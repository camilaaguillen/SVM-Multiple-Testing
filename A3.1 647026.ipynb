{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2269b81b-77d3-4623-91b2-f433228fb54a",
   "metadata": {},
   "source": [
    "1. Importamos los datos a nuestro ambiente de trabajo y revisamos que no haya huecos.\n",
    "Encontramos que hay cero huecos en la base de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e76fd5-cdad-4360-8c1c-9a8957caa5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X1        X2        X3        X4        X5        X6        X7  \\\n",
      "0  0.773344 -2.438405 -0.482562 -2.721135 -1.217058  0.827809  1.342604   \n",
      "1 -0.078178 -2.415754  0.412772 -2.825146 -0.626236  0.054488  1.429498   \n",
      "2 -0.084469 -1.649739 -0.241308 -2.875286 -0.889405 -0.027474  1.159300   \n",
      "3  0.965614 -2.380547  0.625297 -1.741256 -0.845366  0.949687  1.093801   \n",
      "4  0.075664 -1.728785  0.852626  0.272695 -1.841370  0.327936  1.251219   \n",
      "\n",
      "         X8        X9       X10  ...     X2300     X2301     X2302     X2303  \\\n",
      "0  0.057042  0.133569  0.565427  ... -0.027474 -1.660205  0.588231 -0.463624   \n",
      "1 -0.120249  0.456792  0.159053  ... -0.246284 -0.836325 -0.571284  0.034788   \n",
      "2  0.015676  0.191942  0.496585  ...  0.024985 -1.059872 -0.403767 -0.678653   \n",
      "3  0.819736 -0.284620  0.994732  ...  0.357115 -1.893128  0.255107  0.163309   \n",
      "4  0.771450  0.030917  0.278313  ...  0.061753 -2.273998 -0.039365  0.368801   \n",
      "\n",
      "      X2304     X2305     X2306     X2307     X2308  y  \n",
      "0 -3.952845 -5.496768 -1.414282 -0.647600 -1.763172  2  \n",
      "1 -2.478130 -3.661264 -1.093923 -1.209320 -0.824395  2  \n",
      "2 -2.939352 -2.736450 -1.965399 -0.805868 -1.139434  2  \n",
      "3 -1.021929 -2.077843 -1.127629  0.331531 -2.179483  2  \n",
      "4 -2.566551 -1.675044 -1.082050 -0.965218 -1.836966  2  \n",
      "\n",
      "[5 rows x 2309 columns]\n",
      "Cantidad de ceros en la base de datos: \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #Importar librería\n",
    "df=pd.read_csv(\"A3.1 Khan.csv\") #Importar base de datos\n",
    "print(df.head()) #Imprimir primeros valores para revisar\n",
    "print(\"Cantidad de ceros en la base de datos: \")\n",
    "print(df.isnull().sum().sum()) #Contamos los valores huecos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd197886-93d3-4502-a65a-c8160488be24",
   "metadata": {},
   "source": [
    "Calculamos la diferencia de promedios entre las clases 2 y 4 para todos los genes, e imprimimos los 10 genes con la mayor diferencia de medias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863e67ed-14ca-4b18-b9cb-2d7f2df69c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 3 1]\n",
      "\n",
      "Top 10 genes con mayor diferencia de medias entre clases 2 y 4:\n",
      "X187     3.323151\n",
      "X509     2.906537\n",
      "X2046    2.424515\n",
      "X2050    2.401783\n",
      "X129     2.165185\n",
      "X1645    2.065460\n",
      "X1319    2.045941\n",
      "X1955    2.037340\n",
      "X1003    2.011337\n",
      "X246     1.837830\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['y'].unique()) #Encontramos las clases que hay\n",
    "genes = df.drop(columns=['y']) #Seleccionamos todo el df menos la clase\n",
    "mean_class2 = genes[df['y'] == 2].mean() #Calculamos promedio de los genes para toda la clase 2 \n",
    "mean_class4 = genes[df['y'] == 4].mean()#Calculamos promedio de los genes para toda la clase 4 \n",
    "diff = (mean_class2 - mean_class4).abs() #Calculamos la diferencia \n",
    "top10 = diff.sort_values(ascending=False).head(10) #Ordenamos de mayor a menor\n",
    "print(\"\\nTop 10 genes con mayor diferencia de medias entre clases 2 y 4:\")\n",
    "print(top10) #Imprimimos primeros 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cee489-7fa0-41fb-bda9-cf4b91a15143",
   "metadata": {},
   "source": [
    "La diferencia en el promedio de expresión de cada gen entre las clases 2 y 4 indica que ciertos genes presentan valores más altos en una clase que en la otra. En términos de inferencia, esto puede ser un indicio de que dichos genes están relacionados con los factores que permiten distinguir o clasificar las muestras entre las clases 2 y 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90d311-55ed-46f2-a650-0a655262f4af",
   "metadata": {},
   "source": [
    "2. Calculamos el estadístico t y el p-value para comparar las medias de todos los genes entre la clase 2 y la clase 4 de la base de datos. Usamos la metodología de Bonferroni, de Holm, y de Benjamini-Hochberg para corregir por múltiples pruebas e indicamos, para cada una, qué genes tienen una expresión significativamente distinta entre las clases (maneja un controlde 0.05). Usamos la función multipletests de statsmodels.stats.multitest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b524707-c4e2-46d8-9078-bc4cf7cdba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes significativamente distintos (Bonferroni):\n",
      "       gene       p_value  bonferroni_p\n",
      "1        X2  7.383962e-08  1.704218e-04\n",
      "35      X36  7.885432e-07  1.819958e-03\n",
      "66      X67  1.413077e-05  3.261383e-02\n",
      "128    X129  2.516574e-09  5.808252e-06\n",
      "173    X174  5.603441e-09  1.293274e-05\n",
      "...     ...           ...           ...\n",
      "2045  X2046  1.769295e-14  4.083533e-11\n",
      "2049  X2050  4.084836e-15  9.427801e-12\n",
      "2114  X2115  7.286580e-06  1.681743e-02\n",
      "2145  X2146  7.001831e-10  1.616023e-06\n",
      "2246  X2247  3.066047e-07  7.076436e-04\n",
      "\n",
      "[72 rows x 3 columns]\n",
      "\n",
      "Genes significativamente distintos (Holm):\n",
      "       gene       p_value        holm_p\n",
      "1        X2  7.383962e-08  1.682067e-04\n",
      "35      X36  7.885432e-07  1.786050e-03\n",
      "66      X67  1.413077e-05  3.168120e-02\n",
      "128    X129  2.516574e-09  5.760437e-06\n",
      "173    X174  5.603441e-09  1.282067e-05\n",
      "...     ...           ...           ...\n",
      "2045  X2046  1.769295e-14  4.072917e-11\n",
      "2049  X2050  4.084836e-15  9.419631e-12\n",
      "2114  X2115  7.286580e-06  1.640938e-02\n",
      "2145  X2146  7.001831e-10  1.604820e-06\n",
      "2246  X2247  3.066047e-07  6.959926e-04\n",
      "\n",
      "[72 rows x 3 columns]\n",
      "\n",
      "Genes significativamente distintos (Benjamini-Hochberg):\n",
      "       gene       p_value      bh_p\n",
      "1        X2  7.383962e-08  0.000005\n",
      "2        X3  9.621623e-05  0.002343\n",
      "28      X29  1.113505e-03  0.014121\n",
      "35      X36  7.885432e-07  0.000041\n",
      "51      X52  4.065804e-04  0.006427\n",
      "...     ...           ...       ...\n",
      "2277  X2278  6.339875e-04  0.009145\n",
      "2294  X2295  3.283459e-04  0.005413\n",
      "2299  X2300  2.057612e-04  0.003957\n",
      "2300  X2301  2.017711e-04  0.003913\n",
      "2302  X2303  2.491232e-05  0.000777\n",
      "\n",
      "[296 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "p_values = []\n",
    "t_stats = []\n",
    "class2 = genes[df['y'] == 2]\n",
    "class4 = genes[df['y'] == 4]\n",
    "for gen in genes.columns:\n",
    "    t_stat, p_val = ttest_ind(class2[gen], class4[gen], equal_var=False)\n",
    "    t_stats.append(t_stat)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Crear un DataFrame con los resultados iniciales\n",
    "results = pd.DataFrame({\n",
    "    'gene': genes.columns,\n",
    "    't_stat': t_stats,\n",
    "    'p_value': p_values\n",
    "})\n",
    "alpha = 0.05\n",
    "\n",
    "# Bonferroni\n",
    "results['bonferroni_corrected'], results['bonferroni_p'], _, _ = multipletests(results['p_value'], alpha=alpha, method='bonferroni')\n",
    "# Holm\n",
    "results['holm_corrected'], results['holm_p'], _, _ = multipletests(results['p_value'], alpha=alpha, method='holm')\n",
    "# Benjamini-Hochberg (FDR)\n",
    "results['bh_corrected'], results['bh_p'], _, _ = multipletests(results['p_value'], alpha=alpha, method='fdr_bh')\n",
    "\n",
    "# Filtrar genes significativos por cada método\n",
    "sig_bonf = results[results['bonferroni_corrected']]\n",
    "sig_holm = results[results['holm_corrected']]\n",
    "sig_bh = results[results['bh_corrected']]\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Genes significativamente distintos (Bonferroni):\")\n",
    "print(sig_bonf[['gene', 'p_value', 'bonferroni_p']])\n",
    "\n",
    "print(\"\\nGenes significativamente distintos (Holm):\")\n",
    "print(sig_holm[['gene', 'p_value', 'holm_p']])\n",
    "\n",
    "print(\"\\nGenes significativamente distintos (Benjamini-Hochberg):\")\n",
    "print(sig_bh[['gene', 'p_value', 'bh_p']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccfa173-5404-4b36-968f-2acfc87566cd",
   "metadata": {},
   "source": [
    "3. Realizamos un experimento similar, pero ahora comparando las medias de las 4 clases de labase de datos. Para lograrlo, en vez de trabajar con el estadístico t, es recomendado realizarvpruebas de análisis de varianza (ANOVA). Dicha prueba se puede realizar con la función f_oneway de scipy.stats, pero debemos revisar bien cómo se deben ingresar los datos a dicha función, necesitarermos primero estratificarlos por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438cab92-58e2-47da-85df-914d32237214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Genes significativamente distintos entre las 4 clases (Bonferroni):\n",
      "   gene       p_value  bonferroni_p\n",
      "0    X1  3.839240e-20  8.860966e-17\n",
      "1    X2  1.977997e-13  4.565218e-10\n",
      "2    X3  5.004749e-07  1.155096e-03\n",
      "16  X17  8.650499e-07  1.996535e-03\n",
      "28  X29  7.937953e-09  1.832080e-05\n",
      "\n",
      "Genes significativamente distintos entre las 4 clases (Holm):\n",
      "   gene       p_value        holm_p\n",
      "0    X1  3.839240e-20  8.837930e-17\n",
      "1    X2  1.977997e-13  4.456428e-10\n",
      "2    X3  5.004749e-07  1.031479e-03\n",
      "16  X17  8.650499e-07  1.767297e-03\n",
      "28  X29  7.937953e-09  1.706660e-05\n",
      "\n",
      "Genes significativamente distintos entre las 4 clases (Benjamini-Hochberg):\n",
      "   gene       p_value          bh_p\n",
      "0    X1  3.839240e-20  1.265852e-17\n",
      "1    X2  1.977997e-13  8.152174e-12\n",
      "2    X3  5.004749e-07  4.657645e-06\n",
      "8    X9  8.500660e-05  4.036939e-04\n",
      "11  X12  2.418413e-02  4.820118e-02\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "f_stats = []\n",
    "p_values = []\n",
    "classes = df['y'].unique()\n",
    "groups = [genes[df['y'] == c] for c in classes]\n",
    "for gen in genes.columns:\n",
    "    # Extraer la expresión del gen en cada clase\n",
    "    samples = [g[gen] for g in groups]\n",
    "    \n",
    "    # Prueba ANOVA one way\n",
    "    f_stat, p_val = f_oneway(*samples)\n",
    "    \n",
    "    f_stats.append(f_stat)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Crear un dataframe con los resultados\n",
    "anova_results = pd.DataFrame({\n",
    "    'gene': genes.columns,\n",
    "    'F_stat': f_stats,\n",
    "    'p_value': p_values\n",
    "})\n",
    "# Aplicar correcciones por pruebas múltiples\n",
    "alpha = 0.05\n",
    "anova_results['bonferroni_sig'], anova_results['bonferroni_p'], _, _ = multipletests(anova_results['p_value'], alpha=alpha, method='bonferroni')\n",
    "anova_results['holm_sig'], anova_results['holm_p'], _, _ = multipletests(anova_results['p_value'], alpha=alpha, method='holm')\n",
    "anova_results['bh_sig'], anova_results['bh_p'], _, _ = multipletests(anova_results['p_value'], alpha=alpha, method='fdr_bh')\n",
    "\n",
    "# Mostrar los genes significativos según cada método\n",
    "print(\"\\nGenes significativamente distintos entre las 4 clases (Bonferroni):\")\n",
    "print(anova_results[anova_results['bonferroni_sig']][['gene', 'p_value', 'bonferroni_p']].head())\n",
    "\n",
    "print(\"\\nGenes significativamente distintos entre las 4 clases (Holm):\")\n",
    "print(anova_results[anova_results['holm_sig']][['gene', 'p_value', 'holm_p']].head())\n",
    "\n",
    "print(\"\\nGenes significativamente distintos entre las 4 clases (Benjamini-Hochberg):\")\n",
    "print(anova_results[anova_results['bh_sig']][['gene', 'p_value', 'bh_p']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d356b-4c64-4654-9b9f-d286dcca5012",
   "metadata": {},
   "source": [
    "4. Separamos los datos en entrenamiento y prueba, construimos y entrenamos un modelo de SVM con un kernel lineal, con un kernel polinomial de orden 3, y con un kernel radial. Para evitar que el tiempo de procesamiento sea exagerado, seleccionamos solamente algunas variables, partiendo de los resultados que obtuvimos en los puntos anteriores. Esta no es una práctica adecuada, pues estamos cayendo en una situación de fuga de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e340e3f1-e3b1-4c06-aacf-59ab1ebc38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "selected_genes = anova_results.sort_values('p_value').head(10)['gene']\n",
    "X = df[selected_genes]\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "svm_poly = SVC(kernel='poly', degree=3, random_state=42)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "y_pred_poly = svm_poly.predict(X_test)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ab9d1-dc63-4d58-8748-3274ee709502",
   "metadata": {},
   "source": [
    "5. Calculamos para los 3 modelos , las métricas que consideramos importantes para comparar los desempeños. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56545408-442e-48ae-9c63-71fa06b31126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SVM con kernel lineal\n",
      "Accuracy: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      0.89      0.94         9\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.96        25\n",
      "   macro avg       0.97      0.97      0.97        25\n",
      "weighted avg       0.96      0.96      0.96        25\n",
      "\n",
      "\n",
      " SVM con kernel polinomial (grado 3)\n",
      "Accuracy: 0.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      0.89      0.94         9\n",
      "           3       1.00      0.60      0.75         5\n",
      "           4       0.73      1.00      0.84         8\n",
      "\n",
      "    accuracy                           0.88        25\n",
      "   macro avg       0.93      0.87      0.88        25\n",
      "weighted avg       0.91      0.88      0.88        25\n",
      "\n",
      "\n",
      " SVM con kernel radial (RBF)\n",
      "Accuracy: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      0.89      0.94         9\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.96        25\n",
      "   macro avg       0.97      0.97      0.97        25\n",
      "weighted avg       0.96      0.96      0.96        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"\\n SVM con kernel lineal\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_linear))\n",
    "print(classification_report(y_test, y_pred_linear))\n",
    "print(\"\\n SVM con kernel polinomial (grado 3)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_poly))\n",
    "print(classification_report(y_test, y_pred_poly))\n",
    "print(\"\\n SVM con kernel radial (RBF)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rbf))\n",
    "print(classification_report(y_test, y_pred_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3195da-f524-4f25-a898-064200aa2697",
   "metadata": {},
   "source": [
    "Al comparar los tres modelos de SVM, se observa que tanto el kernel lineal como el kernel radial (RBF) obtuvieron un accuracy de 0.96, mientras que el kernel polinomial de grado 3 alcanzó un accuracy ligeramente menor (0.88).\n",
    "\n",
    "El kernel lineal y el RBF presentan además valores muy similares de precisión, recall y F1-score, lo que indica que ambos modelos logran una clasificación casi perfecta en las cuatro clases. En cambio, el modelo con kernel polinomial muestra una ligera disminución en el desempeño, particularmente en la clase 3, donde el recall baja a 0.60.\n",
    "\n",
    "Esto sugiere que, para este conjunto de datos, la separación entre clases parece ser casi lineal, o bien, que las fronteras entre clases no requieren una transformación muy compleja del espacio de características. Por ello, tanto el modelo lineal como el radial son adecuados, mientras que el polinomial podría estar sobreajustando ligeramente o introduciendo complejidad innecesaria.\n",
    "\n",
    "En conclusión, el SVM con kernel lineal se perfila como la mejor opción para esta tarea específica, ya que logra una alta precisión con un modelo más simple y menor costo computacional, obteniendo resultados comparables al kernel RBF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5c5ee-ef93-4f3a-9e5c-f2a4520c2721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
